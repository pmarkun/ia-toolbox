from openai import OpenAI
import tiktoken
import subprocess
import os
import streamlit as st
from stqdm import stqdm
from time import sleep

def transcribe_audio(audio_path, options=None):
    """
    Transcribes an audio file using Whisper locally with a progress bar.

    Parameters:
    - audio_path (str): Path to the audio file to be transcribed.
    - options (dict, optional): Dictionary with additional options:
        - 'output_dir' (str): Directory to save the transcription.
        - 'model' (str): Whisper model to be used (default: 'base').
        - 'language' (str): Language of the audio (default: 'pt').

    Returns:
    - str: Transcribed text.
    """
    if options is None:
        options = {}

    output_dir = options.get('output_dir', '.')
    model = options.get('model', 'base')
    language = options.get('language', 'pt')

    # Command to transcribe using Whisper
    command = [
        'whisper', audio_path,
        '--model', model,
        '--language', language,
        '--output_dir', output_dir
    ]

    # Execute the command with a progress bar
    with stqdm(total=100, desc="Transcribing audio") as pbar:
        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        while process.poll() is None:
            sleep(0.1)
            pbar.update(1)
        stdout, stderr = process.communicate()

    # Check for errors
    if process.returncode != 0:
        raise Exception(f"Transcription error: {stderr.decode()}")

    # Name of the generated transcription file
    base_name = os.path.splitext(os.path.basename(audio_path))[0]
    transcription_file = os.path.join(output_dir, f"{base_name}.txt")

    # Read and return the transcription content
    with open(transcription_file, 'r') as f:
        transcription = f.read()

    return transcription

def split_text(text, max_tokens, encoding):
    """
    Splits the text into parts that do not exceed max_tokens, keeping whole words.

    Parameters:
    - text (str): Text to be split.
    - max_tokens (int): Maximum number of tokens per part.
    - encoding: tiktoken encoding object.

    Returns:
    - List[str]: List of text parts.
    """
    words = text.split()
    parts = []
    current_part = []

    for word in words:
        current_part.append(word)
        part_tokens = len(encoding.encode(' '.join(current_part)))
        if part_tokens > max_tokens:
            current_part.pop()  # Remove the last word that exceeded the limit
            parts.append(' '.join(current_part))
            current_part = [word]

    if current_part:
        parts.append(' '.join(current_part))

    return parts

def process_prompt(data, options=None):
    """
    Sends parts of the content to ChatGPT and returns the concatenated response.

    Parameters:
    - data (dict): Dictionary with 'prompt' and 'content' keys.
    - options (dict, optional): Dictionary with additional options:
        - 'api_key' (str): OpenAI API key.
        - 'model' (str): ChatGPT model to be used (default: 'gpt-4').
        - 'max_tokens' (int): Maximum number of tokens in the response (default: 3000).
        - 'temperature' (float): Degree of randomness in the response (default: 1.0).

    Returns:
    - str: Concatenated response generated by ChatGPT.
    """
    if options is None:
        options = {}

    api_key = options.get('api_key')
    model = options.get('model', 'gpt-4o-mini')
    max_tokens = options.get('max_tokens', 16384)
    temperature = options.get('temperature', 1.0)

    if not api_key:
        raise ValueError("API key is required.")
    client = OpenAI(api_key=api_key)

    encoding = tiktoken.encoding_for_model(model)
    max_tokens_per_part = int(max_tokens * 0.5)

    content_parts = split_text(data['content'], max_tokens_per_part, encoding)
    final_result = ""

    with stqdm(total=len(content_parts), desc="Processing parts with ChatGPT") as pbar:
        for part in content_parts:
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "system",
                        "content": [
                            {
                                "type": "text",
                                "text": data['sys_prompt']
                            }
                        ],

                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": part
                            }
                        ],
                        
                    }
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            final_result += response.choices[0].message.content.strip() + "\n"
            pbar.update(1)

    return final_result
